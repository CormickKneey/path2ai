{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebbfca02-27c6-4891-add1-02fe81b772ca",
   "metadata": {},
   "source": [
    "# 马尔可夫决策过程\n",
    "\n",
    "利用强化学习解决实际问题，第一步要做的事情就是把实际问题抽象为马尔可夫决策过程，也就是明确马尔可夫决策过程的各个组成要素，因此马尔可夫决策过程（Markov decision process，MDP）是强化学习的重要概念。。\n",
    "\n",
    "## 关键概念与定义\n",
    "\n",
    "### 马尔可夫性质\n",
    "\n",
    "当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有马尔可夫性质（Markov property），用公式表示为。也就是说，当前状态是未来的充分统计量，即下一个状态只取决于当前状态，而不会受到过去状态的影响。需要明确的是，具有马尔可夫性并不代表这个随机过程就和历史完全没有关系。因为虽然时刻的状态只与时刻的状态有关，但是时刻的状态其实包含了时刻的状态的信息，通过这种链式的关系，历史的信息被传递到了现在。马尔可夫性可以大大简化运算，因为只要当前状态可知，所有的历史信息都不再需要了，利用当前状态信息就可以决定未来。\n",
    "\n",
    "举例子, 我们定义一个状态转移的规律如下: \n",
    "\n",
    "|         | Winning | Losing | Drawing |\n",
    "|---------|---------|--------|---------|\n",
    "| Winning | 0.6     | 0.1    | 0.3     |\n",
    "| Losing  | 0.3     | 0.4    | 0.3     |\n",
    "| Drawing | 0.4     | 0.2    | 0.4     |\n",
    "\n",
    "> 球队本场state到下场state的概率\n",
    "\n",
    "如果该球队胜利了，那么两场比赛后的状态如下:\n",
    "Winning = (0.6 * 0.6) + (0.1 * 0.3) + (0.3 * 0.4) = 0.36 + 0.03 + 0.12 = 0.51\n",
    "Losing = (0.6 * 0.1) + (0.1 * 0.4) + (0.3 * 0.2) = 0.06 + 0.04 + 0.06 = 0.16\n",
    "Drawing = (0.6 * 0.3) + (0.1 * 0.3) + (0.3 * 0.4) = 0.18 + 0.03 + 0.12 = 0.33\n",
    "\n",
    "这样我们就完成了一个符合马尔可夫性质的建模。\n",
    "\n",
    "### 马尔可夫奖励过程（Markov Reward Process, MRP）\n",
    "\n",
    "描述了在满足马尔可夫性质的序列中，每个状态附带奖励的数学框架。它是马尔可夫决策过程（MDP）的前置概念，去除了“动作”部分，仅关注状态转移的随机性和奖励的期望。它的建模包含了以下的定义:\n",
    "\n",
    "| **符号**       | **名称**                | **数学定义**                                                                 |\n",
    "|----------------|-------------------------|----------------------------------------------------------------------------|\n",
    "| $\\mathcal{S}$  | 状态集合                | 所有可能的状态 $s \\in \\mathcal{S}$                                        |\n",
    "| $\\mathcal{P}$  | 状态转移概率矩阵        | $\\mathcal{P}(s' \\| s) = \\mathbb{P}(S_{t+1}=s' \\| S_t = s)$                |\n",
    "| $\\mathcal{R}$  | 奖励函数                | $\\mathcal{R}(s) = \\mathbb{E}[R_{t+1} \\| S_t = s]$                         |\n",
    "| $\\gamma$       | 折扣因子                | $0 \\leq \\gamma \\leq 1$，控制未来奖励的权重                                |\n",
    "\n",
    "\n",
    "#### 回报\n",
    "在符合马尔可夫性质的过程中，所有奖励的衰减之和称为回报，现在为我们的球队的结果设置一个奖励矩阵吧:\n",
    "\n",
    "|         | Winning | Losing | Drawing |\n",
    "|---------|---------|--------|---------|\n",
    "| Winning | +10     | -5     | 0       |\n",
    "| Losing  | +12      | 0      | +2      |\n",
    "| Drawing | +5      | -3     | 0       |\n",
    "\n",
    "如此一来, Winning -> Winning -> Losing 的回报就等于: 10 + （-5） = 5\n",
    "\n",
    "#### 价值函数\n",
    "\n",
    "在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的价值（value）。所有状态的价值就组成了价值函数（value function），价值函数的输入为某个状态，输出为这个状态的价值。我们将价值函数写成: \n",
    "\n",
    "$$V(s) = \\mathbb{E} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\,\\bigg|\\, S_t = s \\right]$$\n",
    "- **符号说明**  \n",
    "  - $\\gamma$：折扣因子，$0 \\leq \\gamma < 1$  \n",
    "  - $R_{t+k+1}$：时间步 $t+k+1$ 的即时奖励\n",
    "\n",
    "也可以写作**贝尔曼方程**:\n",
    "\n",
    "$$V(s) = \\underbrace{\\sum_{s'} P(s' | s) R(s, s')}_{\\text{即时奖励期望-r(s)}} + \\gamma \\underbrace{\\sum_{s'} P(s' | s) V(s')}_{\\text{未来价值期望}}$$\n",
    "\n",
    "- **分解含义**  \n",
    "  - **第一部分**：从状态 \\(s\\) 转移到所有可能 \\(s'\\) 的即时奖励期望。  \n",
    "  - **第二部分**：转移到后续状态后的未来价值期望，按 $\\gamma$ 衰减。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a7bb106-0f58-4e92-9c01-6853a36f7fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态价值函数 (迭代法):\n",
      "  Winning: 34.70\n",
      "  Losing: 36.05\n",
      "  Drawing: 31.85\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MarkovRewardProcess:\n",
    "    def __init__(self, states, P, R, gamma=0.9):\n",
    "        \"\"\"\n",
    "        初始化马尔可夫奖励过程\n",
    "        :param states: 状态名称列表，例如 [\"Winning\", \"Losing\", \"Drawing\"]\n",
    "        :param P: 状态转移概率矩阵，shape=(n_states, n_states)\n",
    "        :param R: 奖励矩阵，R[s][s']表示从状态s转移到s'的即时奖励\n",
    "        :param gamma: 折扣因子，默认为0.9\n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "        self.P = np.array(P)\n",
    "        self.R = np.array(R)\n",
    "        self.gamma = gamma\n",
    "        self.n_states = len(states)\n",
    "        \n",
    "        # 验证输入合法性\n",
    "        assert self.P.shape == (self.n_states, self.n_states), \"P矩阵维度不匹配\"\n",
    "        assert self.R.shape == (self.n_states, self.n_states), \"R矩阵维度不匹配\"\n",
    "        for row in self.P:\n",
    "            assert np.isclose(row.sum(), 1.0), \"状态转移概率之和必须为1\"\n",
    "\n",
    "    def calculate_value_function(self, max_iter=1000, tol=1e-6):\n",
    "        \"\"\"\n",
    "        通过迭代法求解状态价值函数 V\n",
    "        :param max_iter: 最大迭代次数\n",
    "        :param tol: 收敛阈值\n",
    "        :return: 状态价值向量，V[s]对应状态s的价值\n",
    "        \"\"\"\n",
    "        V = np.zeros(self.n_states)  # 初始化价值函数\n",
    "        \n",
    "        for _ in range(max_iter):\n",
    "            V_new = np.zeros(self.n_states)\n",
    "            for s in range(self.n_states):\n",
    "                # 计算每个状态的期望奖励和未来价值\n",
    "                reward = np.sum(self.P[s] * self.R[s])  # 期望即时奖励 R(s)\n",
    "                future_value = np.sum(self.P[s] * V)       # 未来价值的期望\n",
    "                V_new[s] = reward + self.gamma * future_value\n",
    "                \n",
    "            if np.max(np.abs(V - V_new)) < tol:\n",
    "                break\n",
    "            V = V_new.copy()\n",
    "        \n",
    "        return V    \n",
    "\n",
    "states = [\"Winning\", \"Losing\", \"Drawing\"]\n",
    "\n",
    "# 状态转移概率矩阵 P\n",
    "# P[s][s']表示从状态s转移到s'的概率\n",
    "P = [\n",
    "    [0.5, 0.2, 0.3],  # Winning -> (Winning, Losing, Drawing)\n",
    "    [0.4, 0.2, 0.4],  # Losing -> (Winning, Losing, Drawing)\n",
    "    [0.4, 0.2, 0.4]    # Drawing -> (Winning, Losing, Drawing)\n",
    "]\n",
    "\n",
    "# 奖励矩阵 R\n",
    "# R[s][s']表示从状态s转移到s'的即时奖励\n",
    "R = [\n",
    "    [10, -5, 0],   # 从Winning转移到各状态的奖励\n",
    "    [12, 0, 2],   # 从Losing转移到各状态的奖励\n",
    "    [5,  -3, 0]    # 从Drawing转移到各状态的奖励\n",
    "]\n",
    "\n",
    "# 创建MRP并计算价值函数\n",
    "mrp = MarkovRewardProcess(states, P, R, gamma=0.9)\n",
    "V_iterative = mrp.calculate_value_function()\n",
    "\n",
    "# 打印结果\n",
    "print(\"状态价值函数 (迭代法):\")\n",
    "for s, name in enumerate(states):\n",
    "    print(f\"  {name}: {V_iterative[s]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d2a1a3-d889-45ef-b0c3-68d2b58818dd",
   "metadata": {},
   "source": [
    "### 马尔可夫决策过程 (Markov decision process，MDP)\n",
    "\n",
    "马尔可夫决策过程是马尔可夫奖励过程（Markov Reward Process, MRP）的升级形式，在原来**状态转换** 和 **奖励**的基础上加上了**行动(Action,A)** 与 **策略(Policy, P)** 的概念。\n",
    "\n",
    "先来看马尔可夫决策过程的五元组:\n",
    "\n",
    "- $S$: **状态空间（State Space）**，即智能体可能处于的所有状态的集合。\n",
    "- $A$: **动作空间（Action Space）**，即智能体可以执行的所有动作的集合。\n",
    "- $P$: **状态转移概率（Transition Probability）**，$P(s'|s, a)$ 表示在状态 $s$ 采取动作 $a$ 后转移到状态 $s'$ 的概率。\n",
    "- $R$: **奖励函数（Reward Function）**，$R(s, a)$ 表示在状态 $s$ 采取动作 $a$ 后获得的期望奖励。\n",
    "- $\\gamma$: **折扣因子（Discount Factor）**，$0 \\leq \\gamma \\leq 1$，用于平衡当前奖励和未来奖励的重要性。\n",
    "\n",
    "#### 状态转移\n",
    "\n",
    "在 MDP 中，智能体根据某个**策略(Policy)** $\\pi(a|s)$ 选择**动作(Action)**，导致状态的变化：\n",
    "$$\n",
    "s_t \\xrightarrow{a_t} s_{t+1}, \\quad R_t = R(s_t, a_t)\n",
    "$$\n",
    "\n",
    "\n",
    "#### 例子\n",
    "\n",
    "![mdp_example](./images/mdp_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4de297da-99e7-434e-9837-00d86f87bcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "策略1动作价值函数 Q_pi_1(s, a) 矩阵:\n",
      "\t保持s1\t前往s1\t前往s2\t前往s3\t前往s4\t前往s5\t概率前往\n",
      "s1\t-1.61\t0.00\t-0.84\t0.00\t0.00\t0.00\t0.00\n",
      "s2\t0.00\t-1.61\t0.00\t-1.74\t0.00\t0.00\t0.00\n",
      "s3\t0.00\t0.00\t0.00\t0.00\t1.04\t0.00\t0.00\n",
      "s4\t0.00\t0.00\t0.00\t0.00\t0.00\t10.00\t2.15\n",
      "s5\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\n",
      "\n",
      "策略2动作价值函数 Q_pi_2(s, a) 矩阵:\n",
      "\t保持s1\t前往s1\t前往s2\t前往s3\t前往s4\t前往s5\t概率前往\n",
      "s1\t-1.73\t0.00\t-1.05\t0.00\t0.00\t0.00\t0.00\n",
      "s2\t0.00\t-1.73\t0.00\t-2.25\t0.00\t0.00\t0.00\n",
      "s3\t0.00\t0.00\t0.00\t0.00\t-1.01\t0.00\t0.00\n",
      "s4\t0.00\t0.00\t0.00\t0.00\t0.00\t10.00\t1.08\n",
      "s5\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\t0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义状态和动作\n",
    "S = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # 状态空间\n",
    "A = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"]  # 动作空间\n",
    "\n",
    "def join(state, action):\n",
    "    return f\"{state}-{action}\"\n",
    "\n",
    "# 状态转移概率 P(s' | s, a)\n",
    "P = {\n",
    "    \"s1-保持s1-s1\": 1.0,\n",
    "    \"s1-前往s2-s2\": 1.0,\n",
    "    \"s2-前往s1-s1\": 1.0,\n",
    "    \"s2-前往s3-s3\": 1.0,\n",
    "    \"s3-前往s4-s4\": 1.0,\n",
    "    \"s3-前往s5-s5\": 1.0,\n",
    "    \"s4-前往s5-s5\": 1.0,\n",
    "    \"s4-概率前往-s2\": 0.2,\n",
    "    \"s4-概率前往-s3\": 0.4,\n",
    "    \"s4-概率前往-s4\": 0.4,\n",
    "}\n",
    "\n",
    "# 奖励函数 R(s, a)\n",
    "R = {\n",
    "    \"s1-保持s1\": -1,\n",
    "    \"s1-前往s2\": 0,\n",
    "    \"s2-前往s1\": -1,\n",
    "    \"s2-前往s3\": -2,\n",
    "    \"s3-前往s4\": -2,\n",
    "    \"s3-前往s5\": 0,\n",
    "    \"s4-前往s5\": 10,\n",
    "    \"s4-概率前往\": 1,\n",
    "}\n",
    "\n",
    "gamma = 0.5  # 折扣因子\n",
    "\n",
    "# 策略1: 随机策略\n",
    "Pi_1 = {\n",
    "    \"s1-保持s1\": 0.5,\n",
    "    \"s1-前往s2\": 0.5,\n",
    "    \"s2-前往s1\": 0.5,\n",
    "    \"s2-前往s3\": 0.5,\n",
    "    \"s3-前往s4\": 0.5,\n",
    "    \"s3-前往s5\": 0.5,\n",
    "    \"s4-前往s5\": 0.5,\n",
    "    \"s4-概率前往\": 0.5,\n",
    "}\n",
    "\n",
    "Pi_2 = {\n",
    "    \"s1-保持s1\": 0.6,\n",
    "    \"s1-前往s2\": 0.4,\n",
    "    \"s2-前往s1\": 0.3,\n",
    "    \"s2-前往s3\": 0.7,\n",
    "    \"s3-前往s4\": 0.5,\n",
    "    \"s3-前往s5\": 0.5,\n",
    "    \"s4-前往s5\": 0.1,\n",
    "    \"s4-概率前往\": 0.9,\n",
    "}\n",
    "\n",
    "# 计算状态价值函数 V_pi(s)\n",
    "def evaluate_policy(Pi, theta=1e-6, max_iters=1000):\n",
    "    V = {s: 0 for s in S}  # 初始化所有状态的价值为0\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        delta = 0\n",
    "        V_new = V.copy()\n",
    "        \n",
    "        for s in S:\n",
    "            v = 0\n",
    "            for a in A:\n",
    "                key_sa = join(s, a)\n",
    "                if key_sa in Pi:\n",
    "                    action_prob = Pi[key_sa]  # 策略下采取动作 a 的概率\n",
    "                    reward = R.get(key_sa, 0)  # 立即奖励 R(s,a)\n",
    "                    \n",
    "                    expected_value = 0\n",
    "                    for s_next in S:\n",
    "                        key_sas = f\"{key_sa}-{s_next}\"\n",
    "                        transition_prob = P.get(key_sas, 0)\n",
    "                        expected_value += transition_prob * V[s_next]\n",
    "                    \n",
    "                    v += action_prob * (reward + gamma * expected_value)\n",
    "            \n",
    "            V_new[s] = v\n",
    "            delta = max(delta, abs(V_new[s] - V[s]))\n",
    "        \n",
    "        V = V_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return V\n",
    "\n",
    "# 计算动作价值函数 Q_pi(s, a)\n",
    "def compute_q_values(V, Pi):\n",
    "    Q = np.zeros((len(S), len(A)))  # 创建Q值矩阵，行为状态，列为动作\n",
    "    state_idx = {s: i for i, s in enumerate(S)}\n",
    "    action_idx = {a: i for i, a in enumerate(A)}\n",
    "    \n",
    "    for s in S:\n",
    "        for a in A:\n",
    "            key_sa = join(s, a)\n",
    "            reward = R.get(key_sa, 0)\n",
    "            expected_value = 0\n",
    "            \n",
    "            for s_next in S:\n",
    "                key_sas = f\"{key_sa}-{s_next}\"\n",
    "                transition_prob = P.get(key_sas, 0)\n",
    "                expected_value += transition_prob * V[s_next]\n",
    "            \n",
    "            if key_sa in Pi:\n",
    "                Q[state_idx[s], action_idx[a]] = reward + gamma * expected_value\n",
    "    \n",
    "    return Q\n",
    "\n",
    "# 计算状态价值函数 V_pi(s) 和 动作价值函数 Q_pi(s,a)\n",
    "V_pi_1 = evaluate_policy(Pi_1)\n",
    "Q_pi_1 = compute_q_values(V_pi_1, Pi_1)\n",
    "\n",
    "V_pi_2 = evaluate_policy(Pi_2)\n",
    "Q_pi_2 = compute_q_values(V_pi_2, Pi_2)\n",
    "\n",
    "\n",
    "print(\"\\n策略1动作价值函数 Q_pi_1(s, a) 矩阵:\")\n",
    "print(\"\\t\" + \"\\t\".join(A))\n",
    "for i, s in enumerate(S):\n",
    "    row_values = \"\\t\".join(f\"{val:.2f}\" for val in Q_pi_1[i])\n",
    "    print(f\"{s}\\t{row_values}\")\n",
    "\n",
    "print(\"\\n策略2动作价值函数 Q_pi_2(s, a) 矩阵:\")\n",
    "print(\"\\t\" + \"\\t\".join(A))\n",
    "for i, s in enumerate(S):\n",
    "    row_values = \"\\t\".join(f\"{val:.2f}\" for val in Q_pi_2[i])\n",
    "    print(f\"{s}\\t{row_values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b22829-9534-4456-bc8f-9fa10006e69a",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "届此，我们发现**强化学习中的环境就是一个马尔可夫决策过程**，而强化学习的目标就是找到马尔可夫决策过程中的最优策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c5879-4dd4-4968-a700-59bde7d8f5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
